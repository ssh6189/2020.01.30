{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:08.512191Z",
     "start_time": "2020-01-30T01:53:06.507195Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras import backend as K\n",
    "#내가 별도로 설치한 케라스를 사용하겠다.\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:09.302953Z",
     "start_time": "2020-01-30T01:53:08.513189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 12,737\n",
      "Trainable params: 12,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "inputs:  ['dense_input']\n",
      "outputs:  ['output/Sigmoid']\n"
     ]
    }
   ],
   "source": [
    "#XOR문제를 해결하기 위한 데이터\n",
    "#X에는 4*2행렬, 항상 행의 수가 샘플수고, 열의 수가 dimension\n",
    "#Y도 행렬이다.\n",
    "#행렬연산의 결과는 항상 행렬로 나온다.\n",
    "#Y의 구조 행의 수 == 샘플수, 열의 수 == 클래스 수\n",
    "#텐서플로우내부에 들어있는 케라스와 케라스 자체적으로 사용하는것 두가지가 있다. 둘다 다르다.\n",
    "\n",
    "\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]], 'float32')\n",
    "Y = np.array([[0], [1], [1], [0]], 'float32')\n",
    "\n",
    "#무슨 말이냐? 이렇게 쓰면 오류난다. 즉, 통일을 시켜주어야 한다.\n",
    "#model = keras.models.Sequential()\n",
    "#model.add(tf.keras.layers.Dense(64, input_dim=2, activation='relu'))\n",
    "#model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "#이렇게 하면 된다.\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(64, input_dim=2, activation='relu'))\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "#아님 텐서플로우에 내장된것만 사용해도 된다. 통일만 시켜주면 된다.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(64, input_dim=2, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "#맨 마지막은 0~1사이 값이 나오도록, sigmoid 사용\n",
    "#그러면, Y = np.array([[0], [2], [2], [0]])으로 써도 상관없다.\n",
    "#\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "#sigmoid 레이어의 이름이 output으로 바뀐것을 볼 수 있다.\n",
    "model.add(tf.keras.layers.Dense(1, activation ='sigmoid', name = 'output'))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "model.fit(X, Y, batch_size=1, epochs=100, verbose=0)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#생성할때마다 이름이 바뀐다. 다시 실행시 그 다음부터\n",
    "#내가 다른 사람들에게 배포할때는, 이름을 고정시켜 주어야 한다.\n",
    "\n",
    "# inputs:  ['dense_input']\n",
    "print('inputs: ', [input.op.name for input in model.inputs])\n",
    "\n",
    "# outputs:  ['dense_4/Sigmoid']\n",
    "print('outputs: ', [output.op.name for output in model.outputs])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.save('xor.h5')\n",
    "\n",
    "#텐서플로우를 백그라운드로, 케라스는 얹혀진 형태\n",
    "#우리가 케라스로 보고 있는 이름과 실제, 텐서플로우에서 사용하는 이름은 다르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:09.330904Z",
     "start_time": "2020-01-30T01:53:09.303970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00824266],\n",
       "       [0.99328977]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#꺽쇠가 두번이면 행렬, 11일때, 0, 01일때, 1이 나오는것을 확인할 수 있다.\n",
    "model.predict(np.array([[1, 1], [0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:09.416617Z",
     "start_time": "2020-01-30T01:53:09.331940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From <ipython-input-4-8e3aaac0ccc4>:35: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 39 variables.\n",
      "INFO:tensorflow:Converted 39 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./xor.pb'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.compat.v1.keras import backend as K\n",
    "#텐서플로우를 버전1.0을 사용하겠다.\n",
    "import tensorflow.compat.v1 as tf\n",
    "#여기서부터 텐서플로우는 버전1.0이다. 이때, tf는 tensorflow.compat.v1이다.\n",
    "\n",
    "#tf2버전은 사용하지 않겠다.\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#여기는 자세히는 이해 안해도 됨\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = ''\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "\n",
    "\n",
    "#output node만 알고 있으면 된다.\n",
    "frozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model.outputs])\n",
    "#True면, 사람이 읽을 수 있는 텍스트\n",
    "#False면 배열로 표시\n",
    "tf.train.write_graph(frozen_graph, './', 'xor.pbtxt', as_text=True)\n",
    "tf.train.write_graph(frozen_graph, './', 'xor.pb', as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:09.442574Z",
     "start_time": "2020-01-30T01:53:09.418611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dense/MatMul', 'dense/Relu', 'dense_1_1/MatMul', 'dense_1_1/Relu', 'dense_2_1/MatMul', 'dense_2_1/Relu', 'dense_3/MatMul', 'dense_3/Relu', 'output/MatMul', 'output/Sigmoid']\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv2\n",
    "#텐서플로우로부터 가져온 모델을 사용하기 위해 쓰는것\n",
    "net = cv2.dnn.readNetFromTensorflow('xor.pb')\n",
    "#Layer들의 이름을 받아낸것을 변수에 저장\n",
    "#이름/연산 이런 꼴로 나타난다.\n",
    "layersNames = net.getLayerNames()\n",
    "print(layersNames)\n",
    "#실제 레이어는 보기보다 많다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:09.448557Z",
     "start_time": "2020-01-30T01:53:09.443543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01501559]\n",
      " [0.99328977]\n",
      " [0.9908274 ]\n",
      " [0.00824266]]\n"
     ]
    }
   ],
   "source": [
    "net.setInput(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]))\n",
    "\n",
    "#마지막 출력Layer를 보려면, 마지막 행 이름으로 바꾸어 주어야한다.\n",
    "out = net.forward(outputName='output/Sigmoid')\n",
    "print(out)\n",
    "\n",
    "#마지막까지 연산을 해주세요. 거꾸로 가는것은 backward\n",
    "out = net.forward()\n",
    "#print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mnist 로딩\n",
    "- 내가 keras를 pb로 변환해서 동작하는지 확인 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:52.317550Z",
     "start_time": "2020-01-30T01:53:09.449529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 42s 701us/step - loss: 0.2643 - accuracy: 0.9181 - val_loss: 0.0536 - val_accuracy: 0.9828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a43cf41d08>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 1\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:53.672989Z",
     "start_time": "2020-01-30T01:53:52.318545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.05364830540092662\n",
      "Test accuracy: 0.9828000068664551\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:53.782984Z",
     "start_time": "2020-01-30T01:53:53.673987Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('mnist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:55.813491Z",
     "start_time": "2020-01-30T01:53:53.786688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: out\\assets\n"
     ]
    }
   ],
   "source": [
    "# 일단 반드시 이와같이 해야함\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "model3 = tf.keras.models.load_model(\"mnist.h5\", compile=False)\n",
    "model3.save(\"out\", save_format='tf') # 폴더명임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:56.615572Z",
     "start_time": "2020-01-30T01:53:55.814490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: assets\n"
     ]
    }
   ],
   "source": [
    "#model.save(\"mnist2.pb\", save_format='tf')\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "model3 = keras.models.load_model(\"mnist.h5\", compile=False)\n",
    "\n",
    "export_path = 'saved_model.pb가 저장될 디렉토리'\n",
    "model3.save(\"\", save_format='tf')\n",
    "\n",
    "#load_model 을 할 때 두 번째 인자에 compile 옵션을 안 넣으면, 내 모델이 컴파일이 안 되어 있다고 오류가 뜬다. 구글링해보니 컴파일이란 거는 학습할 준비 어쩌고 하던데 하여튼 학습을 하지 않고 추론하는 데에만 모델을 사용할 거라면, compile=False 옵션을 넣어주면 된다고 해서 넣었음.\n",
    "\n",
    "#​\n",
    "\n",
    "#그리고 export_path는 파일이 저장될 디렉토리를 지정해주어야 하는데, 디렉토리 안이 비어있어야 한다.\n",
    "\n",
    "#그리하여 이것을 실행하면 !!\n",
    "#[출처] Keras 모델을 TensorFlow Lite로 변환하기 (h5 -> pb -> tflite)|작성자 aainy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:56.857043Z",
     "start_time": "2020-01-30T01:53:56.616486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 84 variables.\n",
      "INFO:tensorflow:Converted 84 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./mnist.pb'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.compat.v1.keras import backend as K\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model.outputs])\n",
    "#tf.train.write_graph(frozen_graph, './', 'xor.pbtxt', as_text=True)\n",
    "tf.train.write_graph(frozen_graph, './', 'mnist.pb', as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:56.874995Z",
     "start_time": "2020-01-30T01:53:56.858041Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.core import framework\n",
    "\n",
    "def find_all_nodes(graph_def, **kwargs):\n",
    "    for node in graph_def.node:\n",
    "        for key, value in kwargs.items():\n",
    "            if getattr(node, key) != value:\n",
    "                break\n",
    "        else:\n",
    "            yield node\n",
    "    raise StopIteration\n",
    "\n",
    "\n",
    "def find_node(graph_def, **kwargs):\n",
    "    try:\n",
    "        return next(find_all_nodes(graph_def, **kwargs))\n",
    "    except StopIteration:\n",
    "        raise ValueError(\n",
    "            'no node with attributes: {}'.format(\n",
    "                ', '.join(\"'{}': {}\".format(k, v) for k, v in kwargs.items())))\n",
    "\n",
    "\n",
    "def walk_node_ancestors(graph_def, node_def, exclude=set()):\n",
    "    openlist = list(node_def.input)\n",
    "    closelist = set()\n",
    "    while openlist:\n",
    "        name = openlist.pop()\n",
    "        if name not in exclude:\n",
    "            node = find_node(graph_def, name=name)\n",
    "            openlist += list(node.input)\n",
    "            closelist.add(name)\n",
    "    return closelist\n",
    "\n",
    "\n",
    "def remove_nodes_by_name(graph_def, node_names):\n",
    "    for i in reversed(range(len(graph_def.node))):\n",
    "        if graph_def.node[i].name in node_names:\n",
    "            del graph_def.node[i]\n",
    "\n",
    "\n",
    "def make_shape_node_const(node_def, tensor_values):\n",
    "    node_def.op = 'Const'\n",
    "    node_def.ClearField('input')\n",
    "    node_def.attr.clear()\n",
    "    node_def.attr['dtype'].type = framework.types_pb2.DT_INT32\n",
    "    tensor = node_def.attr['value'].tensor\n",
    "    tensor.dtype = framework.types_pb2.DT_INT32\n",
    "    tensor.tensor_shape.dim.add()\n",
    "    tensor.tensor_shape.dim[0].size = len(tensor_values)\n",
    "    for value in tensor_values:\n",
    "        tensor.tensor_content += value.to_bytes(4, 'little')\n",
    "    output_shape = node_def.attr['_output_shapes']\n",
    "    output_shape.list.shape.add()\n",
    "    output_shape.list.shape[0].dim.add()\n",
    "    output_shape.list.shape[0].dim[0].size = len(tensor_values)\n",
    "\n",
    "\n",
    "def make_cv2_compatible(graph_def):\n",
    "    # A reshape node needs a shape node as its second input to know how it\n",
    "    # should reshape its input tensor.\n",
    "    # When exporting a model using Keras, this shape node is computed\n",
    "    # dynamically using `Shape`, `StridedSlice` and `Pack` operators.\n",
    "    # Unfortunately those operators are not supported yet by the OpenCV API.\n",
    "    # The goal here is to remove all those unsupported nodes and hard-code the\n",
    "    # shape layer as a const tensor instead.\n",
    "    for reshape_node in find_all_nodes(graph_def, op='Reshape'):\n",
    "\n",
    "        # Get a reference to the shape node\n",
    "        shape_node = find_node(graph_def, name=reshape_node.input[1])\n",
    "\n",
    "        # Find and remove all unsupported nodes\n",
    "        garbage_nodes = walk_node_ancestors(graph_def, shape_node,\n",
    "                                            exclude=[reshape_node.input[0]])\n",
    "        remove_nodes_by_name(graph_def, garbage_nodes)\n",
    "\n",
    "        # Infer the shape tensor from the reshape output tensor shape\n",
    "        if not '_output_shapes' in reshape_node.attr:\n",
    "            raise AttributeError(\n",
    "                'cannot infer the shape node value from the reshape node. '\n",
    "                'Please set the `add_shapes` argument to `True` when calling '\n",
    "                'the `Session.graph.as_graph_def` method.')\n",
    "        output_shape = reshape_node.attr['_output_shapes'].list.shape[0]\n",
    "        output_shape = [dim.size for dim in output_shape.dim]\n",
    "\n",
    "        # Hard-code the inferred shape in the shape node\n",
    "        make_shape_node_const(shape_node, output_shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:56:22.091801Z",
     "start_time": "2020-01-30T01:56:21.998052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "can't convert negative int to unsigned",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-fd3fc9516155>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgraph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgraph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_variables_to_constants\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmake_cv2_compatible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Print the graph nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-03460a74436e>\u001b[0m in \u001b[0;36mmake_cv2_compatible\u001b[1;34m(graph_def)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Hard-code the inferred shape in the shape node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mmake_shape_node_const\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-03460a74436e>\u001b[0m in \u001b[0;36mmake_shape_node_const\u001b[1;34m(node_def, tensor_values)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensor_values\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_content\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'little'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_output_shapes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0moutput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: can't convert negative int to unsigned"
     ]
    }
   ],
   "source": [
    "sess = K.get_session()\n",
    "graph_def = sess.graph.as_graph_def(add_shapes=True)\n",
    "graph_def = tf.graph_util.convert_variables_to_constants(sess, graph_def, [model.output.name.split(':')[0]])\n",
    "make_cv2_compatible(graph_def)\n",
    "\n",
    "# Print the graph nodes\n",
    "print('\\n'.join(node.name for node in graph_def.node))\n",
    "\n",
    "# Save the graph as a binary protobuf2 file\n",
    "tf.train.write_graph(graph_def, '', 'mnist.pb', as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:56:34.198156Z",
     "start_time": "2020-01-30T01:56:34.176483Z"
    }
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\dnn\\src\\tensorflow\\tf_io.cpp:42: error: (-2:Unspecified error) FAILED: ReadProtoFromBinaryFile(param_file, param). Failed to parse GraphDef file: saved_model.pb in function 'cv::dnn::ReadTFNetParamsFromBinaryFileOrDie'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e26c50dec49a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#net = cv2.dnn.readNetFromTensorflow('MINIST_CNN_frozen_graph.pb')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNetFromTensorflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'saved_model.pb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlayersNames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLayerNames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayersNames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\dnn\\src\\tensorflow\\tf_io.cpp:42: error: (-2:Unspecified error) FAILED: ReadProtoFromBinaryFile(param_file, param). Failed to parse GraphDef file: saved_model.pb in function 'cv::dnn::ReadTFNetParamsFromBinaryFileOrDie'\n"
     ]
    }
   ],
   "source": [
    "#net = cv2.dnn.readNetFromTensorflow('MINIST_CNN_frozen_graph.pb')\n",
    "import cv2 as cv2\n",
    "net = cv2.dnn.readNetFromTensorflow('saved_model.pb')\n",
    "layersNames = net.getLayerNames()\n",
    "print(layersNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:56:36.537164Z",
     "start_time": "2020-01-30T01:56:36.533173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 28, 28)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "img = np.zeros((28,28), dtype=np.uint8)  #  이미지 일때는 uint8이어야 한다. 그래야 에러 없다.\n",
    "blob = cv2.dnn.blobFromImage(img) #  학습할때 256으로 여기서 나누어야 한다.\n",
    "print(blob.shape)\n",
    "print(blob.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:56:38.223192Z",
     "start_time": "2020-01-30T01:56:38.205240Z"
    }
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\dnn\\src\\layers\\fully_connected_layer.cpp:154: error: (-215:Assertion failed) srcMat.dims == 2 && srcMat.cols == weights.cols && dstMat.rows == srcMat.rows && dstMat.cols == weights.rows && srcMat.type() == weights.type() && weights.type() == dstMat.type() && srcMat.type() == CV_32F && (biasMat.empty() || (biasMat.type() == srcMat.type() && biasMat.isContinuous() && (int)biasMat.total() == dstMat.cols)) in function 'cv::dnn::FullyConnectedLayerImpl::FullyConnected::run'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-fe07c392d954>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#NHWC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\dnn\\src\\layers\\fully_connected_layer.cpp:154: error: (-215:Assertion failed) srcMat.dims == 2 && srcMat.cols == weights.cols && dstMat.rows == srcMat.rows && dstMat.cols == weights.rows && srcMat.type() == weights.type() && weights.type() == dstMat.type() && srcMat.type() == CV_32F && (biasMat.empty() || (biasMat.type() == srcMat.type() && biasMat.isContinuous() && (int)biasMat.total() == dstMat.cols)) in function 'cv::dnn::FullyConnectedLayerImpl::FullyConnected::run'\n"
     ]
    }
   ],
   "source": [
    "#N: number of images in the batch\n",
    "#H: height of the image\n",
    "#W: width of the image\n",
    "#C: number of channels of the image\n",
    "    \n",
    "#blob = blob.reshape(1, 28,28,1)  #NCHW\n",
    "blob = blob.reshape(1, 1, 28,28)  #NHWC\n",
    "net.setInput(blob)\n",
    "out = net.forward()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:56:40.477412Z",
     "start_time": "2020-01-30T01:56:40.420739Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'cifar10.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d1d66e6710c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cifar10.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'write'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode)\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_is_path_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'cifar10.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('cifar10.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:56:47.358555Z",
     "start_time": "2020-01-30T01:56:43.797523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 84 variables.\n",
      "INFO:tensorflow:Converted 84 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./cifar.pbtxt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model.outputs])\n",
    "tf.train.write_graph(frozen_graph, './', 'cifar.pb', as_text=False)\n",
    "tf.train.write_graph(frozen_graph, './', 'cifar.pbtxt', as_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:56:47.417399Z",
     "start_time": "2020-01-30T01:56:47.359587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv2d_1/convolution', 'conv2d_1/Relu', 'conv2d_2/convolution', 'conv2d_2/Relu', 'max_pooling2d_1/MaxPool', 'flatten_1/Shape', 'flatten_1/strided_slice', 'flatten_1/Prod', 'flatten_1/stack_360', 'flatten_1/Reshape', 'dense_3_1/MatMul', 'dense_3_1/Relu', 'dense_4/MatMul', 'dense_4/Softmax']\n"
     ]
    }
   ],
   "source": [
    "net = cv2.dnn.readNetFromTensorflow('cifar.pb')\n",
    "layersNames = net.getLayerNames()\n",
    "print(layersNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:57.254354Z",
     "start_time": "2020-01-30T01:53:06.533Z"
    }
   },
   "outputs": [],
   "source": [
    "#from keras.utils import to_categorical\n",
    "from keras.datasets import cifar10\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:57.255351Z",
     "start_time": "2020-01-30T01:53:06.534Z"
    }
   },
   "outputs": [],
   "source": [
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:57.256349Z",
     "start_time": "2020-01-30T01:53:06.535Z"
    }
   },
   "outputs": [],
   "source": [
    "img = test_images[0,:,:,:] / 255\n",
    "print(img.shape)\n",
    "\n",
    "plt.imshow(img)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:57.256349Z",
     "start_time": "2020-01-30T01:53:06.536Z"
    }
   },
   "outputs": [],
   "source": [
    "o = model.predict(img.reshape(1,32,32,3))\n",
    "\n",
    "np.argmax(o)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:57.257346Z",
     "start_time": "2020-01-30T01:53:06.537Z"
    }
   },
   "outputs": [],
   "source": [
    "img = (img*255).astype(dtype='uint8')\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(img, 1/255) #  학습할때 256으로 나누었음.\n",
    "print(blob.shape)\n",
    "print(blob.dtype)\n",
    "\n",
    "#why ? 보통 1,32,32,3으로 넘겨야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:57.258343Z",
     "start_time": "2020-01-30T01:53:06.537Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "net.setInput(blob)\n",
    "out = net.forward()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:57.259341Z",
     "start_time": "2020-01-30T01:53:06.538Z"
    }
   },
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "  \"\"\"Extract the sub graph defined by the output nodes and convert \n",
    "  all its variables into constant \n",
    "  Args:\n",
    "      model_dir: the root folder containing the checkpoint state file\n",
    "      output_node_names: a string, containing all the output node's names, \n",
    "                          comma separated\n",
    "                        \"\"\"\n",
    "  if not tf.gfile.Exists(model_dir):\n",
    "    raise AssertionError(\n",
    "      \"Export directory doesn't exists. Please specify an export \"\n",
    "      \"directory: %s\" % model_dir)\n",
    "\n",
    "  if not output_node_names:\n",
    "    print(\"You need to supply the name of a node to --output_node_names.\")\n",
    "    return -1\n",
    "\n",
    "  # We retrieve our checkpoint fullpath\n",
    "  checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "  input_checkpoint = checkpoint.model_checkpoint_path\n",
    "    \n",
    "  # We precise the file fullname of our freezed graph\n",
    "  absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\n",
    "  output_graph = absolute_model_dir + \"/frozen_model.pb\"\n",
    "  print(output_graph)\n",
    "\n",
    "  # We clear devices to allow TensorFlow to control on which device it will load operations\n",
    "  clear_devices = True\n",
    "\n",
    "  # We start a session using a temporary fresh Graph\n",
    "  with tf.Session(graph=tf.Graph()) as sess:\n",
    "    # We import the meta graph in the current default Graph\n",
    "    saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
    "\n",
    "    # We restore the weights\n",
    "    saver.restore(sess, input_checkpoint)\n",
    "\n",
    "    # We use a built-in TF helper to export variables to constants\n",
    "    output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "      sess, # The session is used to retrieve the weights\n",
    "      tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n",
    "      output_node_names.split(\",\") # The output node names are used to select the usefull nodes\n",
    "    ) \n",
    "\n",
    "    # Finally we serialize and dump the output graph to the filesystem\n",
    "    with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "      f.write(output_graph_def.SerializeToString())\n",
    "    print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n",
    "\n",
    "  return output_graph_def\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:57.259341Z",
     "start_time": "2020-01-30T01:53:06.539Z"
    }
   },
   "outputs": [],
   "source": [
    "estimator_model = tf.keras.estimator.model_to_estimator(keras_model = model, model_dir = './k')\n",
    "#k 폴더에 keras폴더에 chekc point 파일 저장함\n",
    "# out이름 어덯게 알지?\n",
    "freeze_graph('./k/keras/',\"activation_3/Sigmoid\")\n",
    "\n",
    "#./k/keras/frozen_model.pb에 생성\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:57.260337Z",
     "start_time": "2020-01-30T01:53:06.540Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T01:53:57.261334Z",
     "start_time": "2020-01-30T01:53:06.541Z"
    }
   },
   "outputs": [],
   "source": [
    "model.layers[7].name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
